Loading dataset..
Training set size: 70000
Test set size: 30000
Training start 
Percentage of completed cycles during threshold: 39.131%
Training end! :)
Training time: 2.868s

Accuracy: 0.706133
--- Sommario Timer ---
+-------------------+----------+-----+--------+----------+--------------------+
| Segmento          | Chiamate | Ore | Minuti |  Secondi | T. Singolo Max (s) |
+-------------------+----------+-----+--------+----------+--------------------+
| treshold: main    |    35504 |   0 |      0 | 1.255750 |           0.017539 |
+-------------------+----------+-----+--------+----------+--------------------+
| treshold: sorting |    35504 |   0 |      0 | 0.757876 |           0.008438 |
+-------------------+----------+-----+--------+----------+--------------------+
| Totale            |    71008 |   0 |      0 | 2.013626 |                --- |
+-------------------+----------+-----+--------+----------+--------------------+




Loading dataset..
Training set size: 3500000
Test set size: 1500000
Training start 
Percentage of completed cycles during threshold: 55.683%
Skipped Cycles: 176339690 over 397908532
Training end! :)
Training time: 2m 14.810s

Accuracy: 0.713987
--- Sommario Timer ---
+-------------------+----------+-----+--------+-----------+--------------------+
| Segmento          | Chiamate | Ore | Minuti |   Secondi | T. Singolo Max (s) |
+-------------------+----------+-----+--------+-----------+--------------------+
| treshold: main    |  1593112 |   0 |      1 | 32.465340 |           0.668127 |
+-------------------+----------+-----+--------+-----------+--------------------+
| treshold: sorting |  1593112 |   0 |      0 | 35.616038 |           0.191581 |
+-------------------+----------+-----+--------+-----------+--------------------+
| Totale            |  3186224 |   0 |      2 |  8.081377 |                --- |
+-------------------+----------+-----+--------+-----------+--------------------+


SENZA TRANSPOSE
Loading dataset..
Training set size: 3500000
Test set size: 1500000
Training start
Training end! :)
Training time: 4m 15.415s

Accuracy: 0.713843

==========================================================================
PERFORMANCE INCREDIBILI 1
Loading dataset..
Training set size: 3500000
Test set size: 1500000
Training start
Training end! :)
Time: 45.585s

Accuracy: 0.764888

Cosa è stato provato (ma che non ha funzionato):

PRE-SORTING DELLE FEATURE
Per ciascuna feature (righe della matrice) viene associato un ordinamento specifico
 dei campioni (ottenuti dal bootstrap sampling) e poi vengono filtrate ad ogni split le sole feature interessate.
 L algoritmo è semplice:
 - pre-sort di tutte le righe tramite indici
 - calcolo threshold e fase di split (left e right)
 - si sceglie la best_feature
 - la best_feature padre viene passata ai nodi figli
 - a quel punto i nodi figli sceglieranno delle features a caso come il padre (metodo: sample_features)
 - prenderanno la best_feature del padre e tramite quella verranno filtrati gli indici effettivi da usare

Questa tecnica del pre-sorting evita di fare continui ordinamenti ma la fase di filtraggio diventa più costosa che mai
in quanto se ho, per esempio, 3M di elementi ogni volta devo ciclare sempre su 3M di elementi. E' vero dunque che evito
il sorting ma ciclare sempre su 3M di elementi per filtrare valori fa un pò schifo.
Costo sorting: O(k log k) dove k è la lunghezza del vettore splittato a tempo t - 1
Costo filtraggio: O(n) dove n è il numero di feature (costante)
Di fatto a tempo t_1 (quindi dopo il primo split) k è al più la metà di n

SALTO DI SORT
Di fatto, se il padre ordina una certa feature f, esegue lo split, passa i dati left e right ai figli e questi scelgono
la stessa feature non serve riordinare. E' stata dunque implementato il passaggio di feature da padre a figlio e il
controllo da parte di quest ultimo che se sceglie la stessa feature del padre, può non ordinare per quella specifica feature.
Analizzando i log è stato notato che la riduzione del numero di sorting, con questa tecnica, non è così impattante..
Su circa 100K sorting effettuati solo 6k vengono saltati ergo lo speedup è praticamente inesistente.

RICORSIONE
Pensavo che la ricorsione dasse stackoverflow su dataset grandi ma così non è stato. Ho implementato la versione iterativa
proprio per questo motivo, tuttavia la versione iterativa è risultata comunque leggermente più efficiente di qualche secondo




Loading dataset..
Training set size: 3500000
Test set size: 1500000
Training start
Transposing..
Training tree n. 1
Training end! :)
Time: 35.105s

Accuracy: 0.76999

COMPUTE THRESHOLD CON FEATURE_LABEL (più cache friendly ma ci stanno le copie)
Qui si vuole dimostrare che rendere il tutto più cache friendly non porta a grandi speedup. Avere il tutto più cache
friendly significherebbe avere multiple copie di X (che contiene milioni di elementi) e questo overhead porta ad uno
speedup di circa 4 secondi sul for del threshold

Loading dataset..
Training set size: 3500000
Test set size: 1500000
Training start
Transposing..
Training tree n. 1
Training end! :)
Time: 1m 14.988s

Accuracy: 0.769965
--- Sommario Timer ---
+--------------------------+----------+-----+--------+-----------+--------------------+
| Segmento                 | Chiamate | Ore | Minuti |   Secondi | T. Singolo Max (s) |
+--------------------------+----------+-----+--------+-----------+--------------------+
| majority                 |    67999 |   0 |      0 |  0.019970 |           0.000009 |
+--------------------------+----------+-----+--------+-----------+--------------------+
| split                    |   177532 |   0 |      0 |  5.681912 |           0.197163 |
+--------------------------+----------+-----+--------+-----------+--------------------+
| threshold                |   288540 |   0 |      0 | 52.719560 |           1.298656 |
+--------------------------+----------+-----+--------+-----------+--------------------+
| treshold: main           |   288540 |   0 |      0 |  1.207688 |           0.017494 |
+--------------------------+----------+-----+--------+-----------+--------------------+
| treshold: sorting        |   288540 |   0 |      0 | 32.947666 |           0.843487 |
+--------------------------+----------+-----+--------+-----------+--------------------+
| threshold: label counts  |   288540 |   0 |      0 |  0.094339 |           0.000019 |
+--------------------------+----------+-----+--------+-----------+--------------------+
| treshold: sorting - copy |   288540 |   0 |      0 | 17.747890 |           0.440891 |
+--------------------------+----------+-----+--------+-----------+--------------------+
| label counts             |   135997 |   0 |      0 |  3.702246 |           0.344522 |
+--------------------------+----------+-----+--------+-----------+--------------------+
| bootstrap                |        1 |   0 |      0 |  0.062078 |           0.062078 |
+--------------------------+----------+-----+--------+-----------+--------------------+
| transpose                |        1 |   0 |      0 | 12.350216 |          12.350216 |
+--------------------------+----------+-----+--------+-----------+--------------------+

VS

Loading dataset..
Training set size: 3500000
Test set size: 1500000
Training start
Transposing..
Training tree n. 1
Training end! :)
Time: 36.180s

Accuracy: 0.76999
--- Sommario Timer ---
+-------------------------+----------+-----+--------+-----------+--------------------+
| Segmento                | Chiamate | Ore | Minuti |   Secondi | T. Singolo Max (s) |
+-------------------------+----------+-----+--------+-----------+--------------------+
| majority                |    67999 |   0 |      0 |  0.019509 |           0.000012 |
+-------------------------+----------+-----+--------+-----------+--------------------+
| split                   |   177532 |   0 |      0 |  0.257991 |           0.002654 |
+-------------------------+----------+-----+--------+-----------+--------------------+
| threshold               |   288540 |   0 |      0 | 19.589147 |           0.385567 |
+-------------------------+----------+-----+--------+-----------+--------------------+
| threshold: label counts |   288540 |   0 |      0 |  0.092318 |           0.000021 |
+-------------------------+----------+-----+--------+-----------+--------------------+
| treshold: main          |   288540 |   0 |      0 |  3.955533 |           0.089520 |
+-------------------------+----------+-----+--------+-----------+--------------------+
| treshold: sorting       |   288540 |   0 |      0 | 14.987766 |           0.299309 |
+-------------------------+----------+-----+--------+-----------+--------------------+
| label counts            |   135997 |   0 |      0 |  3.248979 |           0.348248 |
+-------------------------+----------+-----+--------+-----------+--------------------+
| bootstrap               |        1 |   0 |      0 |  0.062707 |           0.062707 |
+-------------------------+----------+-----+--------+-----------+--------------------+
| transpose               |        1 |   0 |      0 | 12.565595 |          12.565595 |
+-------------------------+----------+-----+--------+-----------+--------------------+

Com'è facile notare, per quanto riguarda la versione più cache friendly, il costo del sorting con pdqsort è passato da 2
secondi a 0.8 secondi (quindi l improvement c'è) ma comunque non raggiunge le performance del radix sort che ordina 3M
di elementi in 0.2 secondi

Inoltre anche ipoteticamente eleminando il costo della copia con qualche accorgimento e ammortizzando il costo dello split
sfruttando la binary search, non si raggiungerebbero comunque le performance della versione con il radix sort

Se usassimo il radix sort sulla versione cache friendly, ipoteticamente si raggiungerebbero performance simili ma al costo
di avere copie su copie di X e y riempiendo la memoria, per questo sono stati introdotti gli indici: meno cache-friendly
ma molto meno impattanti sulla memoria

-------------------------------------------------------------------------------
 srun --mpi=pmix -N 5 -n 5 --cpus-per-task=32 ./mpi --exclusive
 Training time (MAX): 791.019 seconds (13.1836 minutes)
 Accuracy: 0.76999
 F1 (Macro): 0.764014

 Eval time (MAX): 31.7781 seconds (0.529635 minutes)
